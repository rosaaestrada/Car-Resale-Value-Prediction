# -*- coding: utf-8 -*-
"""Car_Resale_Value_Prediction_V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dGMZ9O0EFhm5AQlNfYvrZAF-pl7tYjDj

# Car Resale Value: Prediction & Key Influencing Factors
#### This project explores whether the resale value of a car can be accurately predicted using features like brand, age, mileage, fuel type, and more, while also uncovering which attributes contribute most to price depreciation.

### Research Questions:
- Can we predict the optimal selling price of a car?
- What factors influence resale value?
- Which brands hold their value best?
- Do automatic cars depreciate faster than manual ones?
- Does high mileage equal lower price?
"""

!python --version

!pip install xgboost

import pandas as pd
print(f"pandas version: {pd.__version__}")

import numpy as np
print(f"numpy version: {np.__version__}")

import matplotlib
import matplotlib.pyplot as plt
print(f"matplotlib version: {matplotlib.__version__}")

import seaborn
import seaborn as sns
print(f"seaborn version: {sns.__version__}")

import sklearn
print(f"scikit-learn version: {sklearn.__version__}")

import xgboost
print(f"xgboost version: {xgboost.__version__}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from xgboost import XGBRegressor

"""## Load and Clean Data"""

car = pd.read_csv('CarData.csv')

car

car.shape

car.rename(columns = {
    'Car_Name'      : 'car_name',
    'Year'          : 'year',
    'Selling_Price' : 'selling_price',
    'Present_Price' : 'present_price',
    'Kms_Driven'    : 'kms_driven',
    'Fuel_Type'     : 'fuel_type',
    'Seller_Type'   : 'seller_type',
    'Transmission'  : 'transmission',
    'Owner'         : 'owner'},
           inplace = True)

# lowercase all letters in car_name
car['car_name'] = car['car_name'].str.lower()

car

"""## Check Data Types and Handle Missing Values"""

car.dtypes

car.isnull().sum()

dup_rows = car[car.duplicated()]

print("Number of duplicate rows:", len(dup_rows))

print(dup_rows)

# not duplicate rows--contain different values

"""## Check for Outliers"""

car.describe()

"""### Find the IQR and the cutoffs for the outliers

#### Selling_Price
"""

# calculate the 25th and 75th percentiles
Q1 = car['selling_price'].quantile(0.25)
Q3 = car['selling_price'].quantile(0.75)

# calculate the IQR
IQR = Q3 - Q1

# calculate the cutoff for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Display the results
print(f"25th Percentile (Q1) for Selling Price: {Q1}")
print(f"75th Percentile (Q3) for Selling Price: {Q3}")
print(f"Interquartile Range (IQR): {IQR}")
print(f"Lower bound for outliers: {lower_bound}")
print(f"Upper bound for outliers: {upper_bound}")

# drop outliers
car2 = car[(car['selling_price'] >= lower_bound) &
                                      (car['selling_price'] <= upper_bound)]

# Display the DataFrame after dropping outliers
print("\nDataFrame after dropping outliers:")
print(car2)

"""#### Present_Price"""

# calculate the 25th and 75th percentiles
Q1 = car['present_price'].quantile(0.25)
Q3 = car['present_price'].quantile(0.75)

# calculate the IQR
IQR = Q3 - Q1

# calculate the cutoff for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Display the results
print(f"25th Percentile (Q1) for Present Price: {Q1}")
print(f"75th Percentile (Q3) for Present Price: {Q3}")
print(f"Interquartile Range (IQR): {IQR}")
print(f"Lower bound for outliers: {lower_bound}")
print(f"Upper bound for outliers: {upper_bound}")

# drop outliers
car2 = car[(car['present_price'] >= lower_bound) &
                                      (car['present_price'] <= upper_bound)]

# Display the DataFrame after dropping outliers
print("\nDataFrame after dropping outliers:")
print(car2)

"""#### Kms_Driven"""

# calculate the 25th and 75th percentiles
Q1 = car['kms_driven'].quantile(0.25)
Q3 = car['kms_driven'].quantile(0.75)

# calculate the IQR
IQR = Q3 - Q1

# calculate the cutoff for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Display the results
print(f"25th Percentile (Q1) for Kms Driven: {Q1}")
print(f"75th Percentile (Q3) for Kms Driven: {Q3}")
print(f"Interquartile Range (IQR): {IQR}")
print(f"Lower bound for outliers: {lower_bound}")
print(f"Upper bound for outliers: {upper_bound}")

# drop outliers
car2 = car[(car['kms_driven'] >= lower_bound) &
                                      (car['kms_driven'] <= upper_bound)]

# Display the DataFrame after dropping outliers
print("\nDataFrame after dropping outliers:")
print(car2)

"""#### Owner"""

# calculate the 25th and 75th percentiles
Q1 = car['owner'].quantile(0.25)
Q3 = car['owner'].quantile(0.75)

# calculate the IQR
IQR = Q3 - Q1

# calculate the cutoff for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Display the results
print(f"25th Percentile (Q1) for Owner: {Q1}")
print(f"75th Percentile (Q3) for Owner: {Q3}")
print(f"Interquartile Range (IQR): {IQR}")
print(f"Lower bound for outliers: {lower_bound}")
print(f"Upper bound for outliers: {upper_bound}")

# drop outliers
car2 = car[(car['owner'] >= lower_bound) &
                                      (car['owner'] <= upper_bound)]

# Display the DataFrame after dropping outliers
print("\nDataFrame after dropping outliers:")
print(car2)

"""## Exploratory Data Analysis & Feature Engineering"""

car2

"""#### Car Name"""

# Increase figure size
plt.figure(figsize= (12,6))

# Plot bar chart
car2['car_name'].value_counts().plot(kind= 'bar',
                                     color= ['forestgreen', 'lightblue', 'pink'])
# Add labels and title
plt.xlabel("Car Name")
plt.ylabel("Count")
plt.title("Car Count by Car Name")

# Font size
plt.xticks(fontsize= 8)

# Show plot
plt.show()

"""#### Year"""

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car2['year'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# Add titles and labels
plt.title('Year Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# Show grid
plt.grid(axis= 'y', alpha= 0.75)

# Show plot
plt.show()

"""#### Selling Price"""

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car2['selling_price'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# add titles and labels
plt.title('Selling Price Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# show grid
plt.grid(axis= 'y', alpha= 0.75)

# show plot
plt.show()

"""#### Present Price"""

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car2['present_price'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# add titles and labels
plt.title('Present Price Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# show grid
plt.grid(axis= 'y', alpha= 0.75)

# show plot
plt.show()

"""#### Kms Driven"""

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car2['kms_driven'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# add titles and labels
plt.title('Kms Driven Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# show grid
plt.grid(axis= 'y', alpha= 0.75)

# show plot
plt.show()

"""#### Fuel Type"""

# Increase figure size
plt.figure(figsize= (12,6))

# Plot bar chart
car2['fuel_type'].value_counts().plot(kind= 'bar',
                                     color= ['orange', 'forestgreen', 'pink'])
# Add labels and title
plt.xlabel("Fuel Type")
plt.ylabel("Count")
plt.title("Car Count by Fuel Type")

# Font size
plt.xticks(fontsize= 8)

# Show plot
plt.show()

"""#### Seller Type"""

# Increase figure size
plt.figure(figsize= (12,6))

# Plot bar chart
car2['seller_type'].value_counts().plot(kind= 'bar',
                                     color= ['forestgreen', 'lightblue', 'pink'])
# Add labels and title
plt.xlabel("seller type")
plt.ylabel("Count")
plt.title("Car Count by Seller Type")

# Font size
plt.xticks(fontsize= 8)

# Show plot
plt.show()

"""#### Transmission"""

# Increase figure size
plt.figure(figsize= (12,6))

# Plot bar chart
car2['transmission'].value_counts().plot(kind= 'bar',
                                     color= ['forestgreen', 'lightblue', 'pink'])
# Add labels and title
plt.xlabel("Transmission")
plt.ylabel("Count")
plt.title("Car Count by Transmission")

# Font size
plt.xticks(fontsize= 8)

# Show plot
plt.show()

"""#### Owner"""

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car2['owner'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# add titles and labels
plt.title('Owner Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# show grid
plt.grid(axis= 'y', alpha= 0.75)

# show plot
plt.show()

car2['owner'].unique().tolist()

car2['owner'].value_counts()

# Owner Variable has no variability
car3 = car2.drop(columns= ['owner'], inplace= False)

car3

car3.dtypes

"""## Feature Engineering

#### current_year
"""

# create new feature car_age
current_year = 2025
car3['car_age'] = current_year - car3['year']

car3

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car3['car_age'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# Add titles and labels
plt.title('Car Age Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# Show grid
plt.grid(axis= 'y', alpha= 0.75)

# Show plot
plt.show()

"""#### price_difference"""

# create new feature price_difference
car3['price_difference'] = car3['selling_price'] - car3['present_price']

car3

# Increase figure size
plt.figure(figsize= (10,6))

# Create histogram
plt.hist(car3['price_difference'],
         bins= 10,    # adjust number of bars in the graph
         color= 'lightblue',
         alpha= 0.7,  # transparency of graph
         edgecolor= 'black')

# Add titles and labels
plt.title('Price Difference Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')

# Show grid
plt.grid(axis= 'y', alpha= 0.75)

# Show plot
plt.show()

"""### Save preprocessed dataset"""

car3.to_csv('Preprocessed_CarData.csv', index = False)

car3.to_csv(r'C:\Users\Alex Estrada\Desktop\Preprocessed_CarData.csv', index = False)

"""## One Hot Encoding"""

car_encoded = pd.get_dummies(car3,
                             columns= ['fuel_type', 'seller_type', 'transmission'],
                             drop_first= True) # drop the first category to avoid multicollinearity

car_encoded

# Convert only Boolean columns (T/F) to Integer (0/1)
car_encoded[car_encoded.select_dtypes(include='bool').columns] = car_encoded.select_dtypes(include='bool').astype(int)

car_encoded

"""#### One Hot Encoding with Car_Name"""

car_encoded_CN = pd.get_dummies(car3,
                                columns= ['car_name','fuel_type', 'seller_type', 'transmission'],
                                drop_first= True)
car_encoded_CN

# Convert only Boolean columns (T/F) to Integer (0/1)
car_encoded_CN[car_encoded_CN.select_dtypes(include='bool').columns] = car_encoded_CN.select_dtypes(include='bool').astype(int)

car_encoded_CN

"""#### save dataset"""

car_encoded.to_csv('ModelReady_CarData.csv', index = False)

car_encoded.to_csv(r'C:\Users\Alex Estrada\Desktop\ModelReady_CarData.csv', index = False)

# Car Encoded with Car Name
car_encoded_CN.to_csv('ModelReady_CarData_CN.csv', index = False)

car_encoded_CN.to_csv(r'C:\Users\Alex Estrada\Desktop\ModelReady_CarData_CN.csv', index = False)

"""## Train/Test Split"""

# Define X and y
X = car_encoded_CN.drop(columns=['selling_price','price_difference'])  # target column
y = car_encoded_CN['selling_price']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Train Models: Linear Regression, Random Forest, XGBoost"""

# Train models
lr = LinearRegression()
rf = RandomForestRegressor(random_state=42)

lr.fit(X_train, y_train)
rf.fit(X_train, y_train)

# Make predictions
lr_preds = lr.predict(X_test)
rf_preds = rf.predict(X_test)

"""#### save train/test dataset"""

X_train.to_csv('X_train.csv', index=False)
X_test.to_csv('X_test.csv', index=False)

y_train.to_csv('y_train.csv', index=False)
y_test.to_csv('y_test.csv', index=False)

X_train.to_csv(r'C:\Users\Alex Estrada\Desktop\X_train.csv', index = False)
X_test.to_csv(r'C:\Users\Alex Estrada\Desktop\X_test.csv', index = False)

y_train.to_csv(r'C:\Users\Alex Estrada\Desktop\y_train.csv', index = False)
y_test.to_csv(r'C:\Users\Alex Estrada\Desktop\y_test.csv', index = False)

"""## Evaluate Models"""

# Evaluation function
def evaluate(y_true, y_pred, name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{name}:\n RMSE: {rmse:.2f}\n MAE: {mae:.2f}\n R²: {r2:.2f}\n")
    return [rmse, mae, r2]

lr_scores = evaluate(y_test, lr_preds, "Linear Regression")
rf_scores = evaluate(y_test, rf_preds, "Random Forest")

# Train and evaluate XGBoost Regressor
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
xgb_preds = xgb.predict(X_test)
xgb_scores = evaluate(y_test, xgb_preds, "XGBoost")

"""## Model Comparison Table: RSME, MAE, and R²"""

# Combine results into a DataFrame
results_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],
    'RMSE': [lr_scores[0], rf_scores[0], xgb_scores[0]],
    'MAE': [lr_scores[1], rf_scores[1], xgb_scores[1]],
    'R²': [lr_scores[2], rf_scores[2], xgb_scores[2]]
})

# Optional: Round the numbers for cleaner presentation
results_df = results_df.round(2)

# Display
results_df

"""#### Random Forest gives the best perfromance
- It explains 72% of the variation in car selling price

## Q1: Can we predict the optimal selling price of a car?
Yes — the model can predict the optimal selling price of a used car with reasonable accuracy. The Random Forest model performed best, with an R² of 0.72, meaning it explains 72% of the variation in selling prices.

* Its average error (MAE) is about 1,000 USD which means predictions are on average within roughly 1,100 USD of the actual price

This level of accuracy is quite useful for pricing guidance, though it is not perfect and could be improved further with more data or tuning.
"""

# Visual Comparison
plt.scatter(y_test, rf_preds)
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.title('Actual vs. Predicted Selling Price (Random Forest)')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.show()

# tight diagonal line shows strong predictive accuracy and good model fit

"""## Q2: What Factors influence resale value?"""

# Sort features by importance
importances = rf.feature_importances_
features = X.columns
sorted_idx = np.argsort(importances)[::-1]

# Set figure size
plt.figure(figsize=(12,6))

# Plot with tighter spacing and smaller labels
plt.bar(range(len(importances)), importances[sorted_idx], align='center')
plt.xticks(range(len(importances)), features[sorted_idx], rotation=90, ha='right', fontsize=10)

# Add title and layout adjustments
plt.title("Feature Importance (Random Forest)", fontsize= 14)
plt.tight_layout()
plt.show()

"""Present price, year, car age, and kms driven are the top influencers of resale value

## Q3: Which brands hold their value best?
"""

# Calculate average value reatined per car brand
brand_avg = car3.groupby('car_name')[['present_price', 'selling_price']].mean()
brand_avg['value_retained_%'] = (brand_avg['selling_price'] / brand_avg['present_price']) * 100
brand_avg = brand_avg.sort_values('value_retained_%', ascending=False)
brand_avg.head(10)

"""* Vitara Brezza holds its value best, reataining 94% of its original price
* Motorcycles and scooters (Activa (91%), TVS (92%), Mojave (93%), and Bajaj (94%) models) also show strong resale value, suggesting consistent demand and low depreciation.
* This insight can help dealerhsips and online marketplaces highlight high-value retaining brands to consumers, increasing buyers and potential profit margins.

## Q4: Do automatic cars depreciate faster than manual ones?
"""

car3['depreciation'] = car3['present_price'] - car3['selling_price']
car3.groupby('transmission')['depreciation'].mean()

# Box Plot
plt.figure(figsize=(7, 5))
sns.boxplot(data=car3, x='transmission', y='depreciation')

plt.title('Depreciation Comparison: Automatic vs. Manual Transmission')
plt.xlabel('Transmission Type')
plt.ylabel('Depreciation (Present Price - Selling Price)')
plt.show

# Manual cars have a shorter box and lower position showing less depreciation
# Automatic cars with a taller box has a higher median and spread of depreciation

"""* On average, Automatic cars lose 5.50 in value, while Manual cars lose 2.57
* A boxplot confirms this pattern, showing higher median and overall depreciation in automatics.
* Therefore, Automatic cars depreciate in value faster on average

## Q5: Does high mileage = lower price?
"""

plt.figure(figsize=(6,5))
sns.scatterplot(data=car3, x='kms_driven', y='selling_price')

plt.title('Relationship Between Mileage and Selling Price')
plt.xlabel('Kiolmeters Driven')
plt.ylabel('Selling Price')
plt.show()

corr = car3['kms_driven'].corr(car3['selling_price'])
print(f"Correlation between kms driven and selling price: {corr:.2f}")

"""**Moderate negative correlation found. suggesting higher mileage generally leads to lower resale value.**

* The correlation between kms_driven and selling_price is 0.03, indicating almost no linear relationship.
* Most cars have mileage under 100,000 km, creating a tight concentration of values and limits the variability.
* A handful of vehicles exceed 100,000 km — including one extreme outlier at 500,000 km — but these are rare and have little effect on overall trends.
* This suggests that other factors (Car age, brand, and present price) play a much stronger role in determining resale value than mileage alone.

## Research Questions
### Can we predict the optimal selling price of a car?
* Yes - the models demonstrate good predictive power, especially **Random Forest** which achieved:
    - R² = 0.72, meaning it explains 72% of the variation in selling prices.
    - MAE ≈ 1.10, suggesting that predicted prices are on average within $1,100 of actual prices.
* This shows that the model can provide reasonably accurate pricing guidance, especially useful for dealerships, platforms, or sellers looking to set competitive yet fair prices.
### What factors influence resale value?
* Present price (original price of the car) — strongest predictor
* Year / Car Age — newer cars retain more value
* Kms Driven — contributes slightly
* Fuel type, seller type, and transmission — have a smaller but relevant impact
    - These findings suggest that resale value is primarily influenced by the car’s original cost, age, and usage, with brand and type also playing roles.
### Which brands hold their value best?
* Vitara Brezza retains ~94% of its original price — highest among all brands
* Motorcycles and scooters also perform very well:
    - Bajaj Avenger, TVS Sport, UM Renegade Mojave, Activa 4G all retain 90%+ of value
* These high-performing brands indicate strong demand and slower depreciation, ideal for buyers prioritizing long-term value.
### Do automatic cars depreciate faster than manual ones?
* Yes
    - Automatic cars lose on average 5.50
    - Manual cars lose 2.57
* The boxplot confirms this — automatics have a higher median depreciation and more variability.
* Automatic cars depreciate more, likely due to higher upfront cost and lower resale demand in some markets.
### Does high mileage equal lower price?
* Surprisingly, not strongly in this dataset
    - Correlation = 0.03, which means almost no linear relationship
    - The scatterplot shows most cars under 100,000 km, limiting variability
    - While there's a visual trend suggesting higher mileage might reduce price, it's not strong statistically
* Mileage alone isn’t a strong determinant — other factors like age, brand, and present price weigh more heavily.
"""

